# Literature Review

Approaches or solutions that have been tried before on similar projects.

**Summary of Each Work**:

- **Source 1**: Food Industry Sales Prediction

  - **[Link](https://www.diva-portal.org/smash/get/diva2:1563491/FULLTEXT01.pdf)**
  - **Objective**: Find what factors are important for the sales; Predict future sales.
  - **Methods**: Explore, Compare and analyse time series forecasting and supervised machine learning models. Five different models were used: Decision tree regression, Random forest regression, Artificial neural networks, Recurrent neural networks and a time series model called Prophet.
  - **Outcomes**: Using a model based on the time series is to be preferred, that is, Prophet and Recurrent neural network. These two models gave the lowest errors and by that, the most accurate results.
  - **Relation to the Project**: 2 aspects related to the project: 1- Find what factors are important for the sales; 2- Predict future sales.

- **Source 2**: Time Series Forecasting (TSF) Using Various Deep Learning Models

  - **[Link](https://arxiv.org/ftp/arxiv/papers/2204/2204.11115.pdf)**
  - **Objective**: Study how the performance of predictive models change as a function of different look-back window sizes and different amounts of time to predict into the future.They also consider the performance of the recent attention-based Transformer models by compare four different deep learning methods (RNN, LSTM, GRU, and Transformer).
  - **Methods**: Train the selected models with different length of sliding time window of size (w) and amounts of time step to predict into the future (k).
  - **Outcomes**: A relationship exist between the performance and the look-back window sizes and the number of predicted time points into the future. Transformer-based models have the best performance
  - **Relation to the Project**: Since our project is related to a time series problem, this can help us to architect our model

- **Source 3**: Time Series Forecasting (TSF) Using Various Deep Learning Models

  - **[Link](https://arxiv.org/ftp/arxiv/papers/2204/2204.11115.pdf)**
  - **Objective**: Study how the performance of predictive models change as a function of different look-back window sizes and different amounts of time to predict into the future.They also consider the performance of the recent attention-based Transformer models by compare four different deep learning methods (RNN, LSTM, GRU, and Transformer).
  - **Methods**: Train the selected models with different length of sliding time window of size (w) and amounts of time step to predict into the future (k).
  - **Outcomes**: A relationship exist between the performance and the look-back window sizes and the number of predicted time points into the future. Transformer-based models have the best performance
  - **Relation to the Project**: Since our project is related to a time series problem, this can help us to architect our model  
